{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "wechat_final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "congressional-fishing",
        "distributed-revolution",
        "uniform-andrew",
        "collectible-detail",
        "improving-timing",
        "simplified-arizona",
        "cooperative-insured",
        "invisible-thumbnail",
        "affecting-bahrain",
        "reduced-airline",
        "comfortable-skill",
        "binding-combination",
        "working-riverside",
        "rural-beverage",
        "appropriate-occurrence",
        "happy-people",
        "obvious-norman",
        "retained-summer",
        "iWcHaEfboyCV",
        "ZBubyBHIP-Dz",
        "o7P4wuPj3Lnh",
        "GwHv_sA-EP96",
        "s2jzb6goAlRW",
        "CQfhiHYk6EgW",
        "b1rcp6ye5R4M",
        "4JDRPOaKUPE3",
        "Ekp5vv_pPPQF",
        "25M_ULlmV-p9",
        "oX98VsJ541Jn",
        "L9qeWfgkMLHD",
        "H-u5B8TJMNl5",
        "NPJwMQxtMbVm",
        "TbfkajbbNBkg",
        "pD-n0Y2WNhxG",
        "9Ba3lTr0OhU8",
        "dZeWG9N9PAlq",
        "uzUxy-_fPgys",
        "GBGoi80GPtZE",
        "s9FS4DnyQA4U",
        "c9WLIckNQ4EA",
        "hh8S2_-KRZca",
        "DaYCic2nRsq7",
        "-viI2TYhUgSk",
        "Pfqn-x50Xl_d",
        "U_ySXw4HX5Mc",
        "hV_6Tpa6YVPJ",
        "lMjpntj3YeUU",
        "7P4FRHsqY_XA",
        "QA8_s13VZkz5"
      ],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU4ttmiDbGlQ"
      },
      "source": [
        "#### Related CTR Papers:\n",
        "\n",
        "1. [DCN] Deep & Cross Network for Ad Click Predictions (Stanford 2017)\n",
        "2. [DIEN] Deep Interest Evolution Network for Click-Through Rate Prediction (Alibaba 2019)\n",
        "3. BPR Bayesian Personalized Ranking from Implicit Feedback\n",
        "4. DCN V2 Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems\n",
        "5. Deep Interest Network for Click-Through Rate Prediction\n",
        "6. Deep Neural Networks for YouTube Recommendations\n",
        "7. deepfm\n",
        "8. maximum margin matrix factorization\n",
        "9. practical-lessons-from-predicting-clicks-on-ads-at-facebook\n",
        "10. Recommender-Systems-[Netflix]\n",
        "11. Rendle2010FM\n",
        "12. Wide_Deep Learning for Recommender Systems"
      ],
      "id": "FU4ttmiDbGlQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm2VrawFVgZh",
        "outputId": "c3b18c14-cd80-4096-aee9-c060a832ba24"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Xm2VrawFVgZh",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "congressional-fishing"
      },
      "source": [
        "#### Libraries"
      ],
      "id": "congressional-fishing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cordless-capital"
      },
      "source": [
        "# coding: utf-8\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from drive.MyDrive.jupyter_notebook.WeChat_Big_Data_Challenge.comm import ACTION_LIST, STAGE_END_DAY, FEA_COLUMN_LIST\n",
        "from drive.MyDrive.jupyter_notebook.WeChat_Big_Data_Challenge.evaluation import uAUC, compute_weighted_score"
      ],
      "id": "cordless-capital",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voluntary-raise"
      },
      "source": [
        "#### Data Sources"
      ],
      "id": "voluntary-raise"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZryH2KQVt2W5"
      },
      "source": [
        "##### Prepare DataFrame"
      ],
      "id": "ZryH2KQVt2W5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "western-regard"
      },
      "source": [
        "feed_embedding_df = pd.read_csv(\"drive/MyDrive/jupyter_notebook/data/wechat_algo_data1/feed_embeddings.csv\")\n",
        "read_comment_offline_df = pd.read_csv(\"drive/MyDrive/jupyter_notebook/data/offline_train/offline_train_read_comment_12_concate_sample.csv\")\n",
        "read_comment_offline_df = read_comment_offline_df.merge(feed_embedding_df, on=\"feedid\")\n",
        "evaluate_df = pd.read_csv(\"drive/MyDrive/jupyter_notebook/data/evaluate/evaluate_all_13_concate_sample.csv\")\n",
        "evaluate_df = evaluate_df.merge(feed_embedding_df, on=\"feedid\")\n",
        "del feed_embedding_df"
      ],
      "id": "western-regard",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ_v7lq7t69K"
      },
      "source": [
        "##### Prepare Dataset"
      ],
      "id": "eJ_v7lq7t69K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rdy6dT7t9Io"
      },
      "source": [
        "SEED = 2021\n",
        "batch_size = 128\n",
        "read_comment_offline_df = read_comment_offline_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "categorical_features = [\"feedid\", \"authorid\", \"bgm_song_id\", \"bgm_singer_id\", \"userid\", \"device\", \"feed_embedding\"]\n",
        "continuous_features = [\n",
        "    \"videoplayseconds\", \"read_commentsum\", \"likesum\", \n",
        "    \"click_avatarsum\", \"forwardsum\", \"commentsum\", \"followsum\", \n",
        "    \"favoritesum\", \"read_commentsum_user\", \"likesum_user\", \"click_avatarsum_user\",\n",
        "    \"forwardsum_user\", \"commentsum_user\", \"followsum_user\", \"favoritesum_user\"\n",
        "]\n",
        "feed_continuous_features = [\n",
        "    \"videoplayseconds\", \"read_commentsum\", \"likesum\", \n",
        "    \"click_avatarsum\", \"forwardsum\", \"commentsum\", \"followsum\", \n",
        "    \"favoritesum\"                        \n",
        "]\n",
        "user_continuous_features = [\n",
        "    \"read_commentsum_user\", \"likesum_user\", \"click_avatarsum_user\",\n",
        "    \"forwardsum_user\", \"commentsum_user\", \"followsum_user\", \"favoritesum_user\"                            \n",
        "]\n",
        "def get_dict_for_dataset(df):\n",
        "    my_dict = {}\n",
        "    for name, values in df.items():\n",
        "        my_dict[name] = values.values\n",
        "    return my_dict"
      ],
      "id": "8Rdy6dT7t9Io",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnsQqyL7o6cR"
      },
      "source": [
        "def preprocess(x, test=False, model=None):\n",
        "    _continuous_features_ = []\n",
        "    for feature in continuous_features:\n",
        "        _continuous_features_.append(x[feature])\n",
        "    for feature in categorical_features:\n",
        "        if feature == \"feed_embedding\":\n",
        "            feed_embedding_str = tf.strings.split(x[feature], \" \")[:-1]\n",
        "            feed_embedding = tf.strings.to_number(feed_embedding_str)\n",
        "            feed_embedding.set_shape([512])\n",
        "        elif feature == \"device\":\n",
        "            device = tf.cast(x[\"device\"], dtype=tf.int32)\n",
        "            device = tf.one_hot(device, 2) \n",
        "        elif feature == \"feedid\":\n",
        "            feedid = x[feature]\n",
        "        elif feature == \"authorid\":\n",
        "            authorid = x[feature]\n",
        "        elif feature == \"bgm_song_id\":\n",
        "            bgm_song_id = x[feature]\n",
        "        elif feature == \"bgm_singer_id\":\n",
        "            bgm_singer_id = x[feature]\n",
        "        elif feature == \"userid\":\n",
        "            userid = x[feature]\n",
        "    if model == \"fm\":\n",
        "        feed_continuous_features = _continuous_features_[0:8]\n",
        "        user_continuous_features = _continuous_features_[8:]\n",
        "        predictors = (feed_continuous_features, user_continuous_features, feedid, authorid, bgm_song_id, bgm_singer_id, userid, device, feed_embedding)\n",
        "    else:\n",
        "        predictors = (_continuous_features_, feedid, authorid, bgm_song_id, bgm_singer_id, userid, device, feed_embedding)\n",
        "    if test:\n",
        "        return predictors\n",
        "    else:\n",
        "        return (predictors, x[\"read_comment\"])\n",
        "def get_dataset(which, df, batch_size=batch_size, cache=False, test=False, model=None):\n",
        "    try:\n",
        "        dict_df = get_dict_for_dataset(df)\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            dict_df\n",
        "        )\n",
        "    except:\n",
        "        del dict_df, dataset\n",
        "    if which == \"train\":\n",
        "        buffer_size = 1000\n",
        "        dataset = dataset.shuffle(buffer_size, seed=SEED)\n",
        "    dataset = dataset.map(lambda x: preprocess(x, test=test, model=model), num_parallel_calls=tf.data.AUTOTUNE, deterministic=(which == \"test\"))\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=(not test))\n",
        "    dataset = dataset.cache() if cache else dataset\n",
        "    dataset = dataset.prefetch(1)\n",
        "    return dataset"
      ],
      "id": "CnsQqyL7o6cR",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-1ogxMAPY2z"
      },
      "source": [
        "#### Models"
      ],
      "id": "i-1ogxMAPY2z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reeLUpuc_gyL"
      },
      "source": [
        "##### Model Components"
      ],
      "id": "reeLUpuc_gyL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fAlgWl2wEWi"
      },
      "source": [
        "def get_metrics(metrics):\n",
        "    if not metrics:\n",
        "        return None\n",
        "    _metrics_ = []\n",
        "    if \"precision\" in metrics:\n",
        "        _metrics_.append(tf.keras.metrics.Precision())\n",
        "    if \"recall\" in metrics:\n",
        "        _metrics_.append(tf.keras.metrics.Recall())\n",
        "    return _metrics_"
      ],
      "id": "1fAlgWl2wEWi",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoIpwQD3wEca"
      },
      "source": [
        "def get_callbacks(callbacks):\n",
        "    callback_set = []\n",
        "    if \"earlystopping\" in callbacks:\n",
        "        C = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', mode='auto', restore_best_weights=True,\n",
        "            patience=2\n",
        "        )\n",
        "        callback_set.append(C)\n",
        "    return callback_set"
      ],
      "id": "UoIpwQD3wEca",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAU5qc8cE7fJ"
      },
      "source": [
        "def print_status_bar(iteration, total, loss, metrics=None):\n",
        "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
        "                         for m in [loss] + (metrics or [])])\n",
        "    end = \"\" if iteration < total else \"\\n\"\n",
        "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
        "          end=end)"
      ],
      "id": "zAU5qc8cE7fJ",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysia30wG_i1B"
      },
      "source": [
        "class vocab_lookup_layer(keras.layers.Layer):\n",
        "    # https://stackoverflow.com/questions/58507400/how-to-use-tf-lookup-tables-with-tensorflow-2-0-keras-and-mlflow\n",
        "    def __init__(self, vocab, num_oov_buckets, **kwargs):\n",
        "      self.vocab = vocab\n",
        "      self.num_oov_buckets = num_oov_buckets\n",
        "      super(vocab_lookup_layer, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "      vocab_initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "        self.vocab, tf.range(len(self.vocab), dtype=tf.int64)\n",
        "      )\n",
        "      self.table = tf.lookup.StaticVocabularyTable(vocab_initializer, self.num_oov_buckets)\n",
        "      self.built = True\n",
        "    def call(self, inputs):\n",
        "      return self.table.lookup(inputs)\n",
        "    def get_config(self):\n",
        "      return {'vocab': self.vocab, 'num_oov_buckets': self.num_oov_buckets}\n",
        "def dense_layer(n_units, previous_output, BN=False, DR=False,\n",
        "                KR=None, name=None, activation=\"relu\",\n",
        "                KI='he_normal'):\n",
        "    layer = keras.layers.Dense(\n",
        "        n_units, \n",
        "        activation=activation, \n",
        "        kernel_initializer=KI,\n",
        "        kernel_regularizer=KR\n",
        "    )\n",
        "    output = layer(previous_output)\n",
        "    if name:\n",
        "        layer._name=name\n",
        "    if DR:\n",
        "        output = keras.layers.Dropout(DR)(output)\n",
        "    if BN:\n",
        "        output = keras.layers.BatchNormalization(momentum=0.999, trainable=True)(output)\n",
        "    return output\n",
        "def embedding_layer(vocab, output_dim=10, name=None, num_oov_buckets=None):\n",
        "    table = vocab_lookup_layer(vocab, num_oov_buckets)\n",
        "    categorical_feature_input = keras.layers.Input(shape=[], dtype=tf.int64, name=name)\n",
        "    indexes = keras.layers.Lambda(lambda c: table(c))(categorical_feature_input)\n",
        "    embeddings = keras.layers.Embedding(input_dim=len(vocab)+num_oov_buckets, output_dim=output_dim)(indexes)\n",
        "    return categorical_feature_input, embeddings"
      ],
      "id": "Ysia30wG_i1B",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7mGq4TY_qqx"
      },
      "source": [
        "##### Wide and Deep Model"
      ],
      "id": "H7mGq4TY_qqx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB0chZK1_LbZ"
      },
      "source": [
        "feedid_vocab = read_comment_offline_df[\"feedid\"].unique()\n",
        "authorid_vocab = read_comment_offline_df[\"authorid\"].unique()\n",
        "bgm_song_id_vocab = read_comment_offline_df[\"bgm_song_id\"].unique()\n",
        "bgm_singer_id_vocab = read_comment_offline_df[\"bgm_singer_id\"].unique()\n",
        "userid_vocab = read_comment_offline_df[\"userid\"].unique()"
      ],
      "id": "OB0chZK1_LbZ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0ZH5d8CtRIe"
      },
      "source": [
        "def deep_model():\n",
        "    keras.backend.clear_session()\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "    feed_embedding_input = keras.layers.Input(shape=[512], name=\"feed_embedding_input\")\n",
        "    feed_embedding_input_h1 = dense_layer(256, feed_embedding_input, name=\"feed_embedding_input_h1\")\n",
        "    feed_embedding_input_h2 = dense_layer(128, feed_embedding_input_h1, name=\"feed_embedding_input_h2\")\n",
        "    feed_embedding_input_h3 = dense_layer(64, feed_embedding_input_h2, name=\"feed_embedding_input_h3\")\n",
        "    feedid_input, feedid_embedding = embedding_layer(feedid_vocab, num_oov_buckets=240000)\n",
        "    authorid_input, authorid_embedding = embedding_layer(authorid_vocab, num_oov_buckets=40000)\n",
        "    bgm_song_id_input, bgm_song_id_embedding = embedding_layer(bgm_song_id_vocab, num_oov_buckets=60000)\n",
        "    bgm_singer_id_input, bgm_singer_id_embedding = embedding_layer(bgm_singer_id_vocab, num_oov_buckets=40000)\n",
        "    userid_input, userid_embedding = embedding_layer(userid_vocab, num_oov_buckets=40000)\n",
        "    device_input = keras.layers.Input(\n",
        "        shape=(2), \n",
        "        name=\"device_input\"\n",
        "    )\n",
        "    c1 = keras.layers.concatenate(\n",
        "        [\n",
        "            feedid_embedding, \n",
        "            authorid_embedding, \n",
        "            bgm_song_id_embedding, \n",
        "            bgm_singer_id_embedding, \n",
        "            userid_embedding,\n",
        "            device_input,\n",
        "            feed_embedding_input_h3\n",
        "        ]\n",
        "    )\n",
        "    h1 = dense_layer(128, c1, name=\"deep_h1\")\n",
        "    h2 = dense_layer(64, h1, name=\"deep_h2\")\n",
        "    h3 = dense_layer(32, h2, name=\"deep_h3\")\n",
        "    output = keras.layers.Dense(1, name=\"deep_model_output\")(h3)\n",
        "    inputs=[\n",
        "          feedid_input,\n",
        "          authorid_input,\n",
        "          bgm_song_id_input,\n",
        "          bgm_singer_id_input,\n",
        "          userid_input,\n",
        "          device_input,\n",
        "          feed_embedding_input\n",
        "    ]\n",
        "    model = keras.models.Model(\n",
        "      inputs=inputs,\n",
        "      outputs=output,\n",
        "      name=\"deep_model\"\n",
        "    )\n",
        "    return inputs, model\n",
        "def wide_model():\n",
        "    keras.backend.clear_session()\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "    wide_features_input = keras.layers.Input(\n",
        "        shape=(len(continuous_features)), \n",
        "        name=\"wide_model_input\"\n",
        "    )\n",
        "    output = keras.layers.Dense(1, name=\"wide_model_output\")(wide_features_input)\n",
        "    model = keras.models.Model(\n",
        "      inputs=wide_features_input,\n",
        "      outputs=output,\n",
        "      name=\"wide_model\"\n",
        "    )\n",
        "    return [wide_features_input], model\n",
        "def wide_and_deep_model():\n",
        "    keras.backend.clear_session()\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "    deep_inputs, deep_model_layer = deep_model()\n",
        "    wide_inputs, wide_model_layer = wide_model()\n",
        "    deep_output = deep_model_layer(deep_inputs)\n",
        "    wide_output = wide_model_layer(wide_inputs)\n",
        "    output = tf.nn.sigmoid(wide_output+deep_output)\n",
        "    inputs = wide_inputs + deep_inputs \n",
        "    model = keras.models.Model(\n",
        "      inputs=inputs,\n",
        "      outputs=output\n",
        "    )\n",
        "    return model"
      ],
      "id": "N0ZH5d8CtRIe",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KizKtc5y_t8V"
      },
      "source": [
        "##### FM Model"
      ],
      "id": "KizKtc5y_t8V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noOiojho_Lio"
      },
      "source": [
        "def fm_model():\n",
        "    keras.backend.clear_session()\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "    feed_embedding_input = keras.layers.Input(shape=[512], name=\"feed_embedding_input\")\n",
        "    feed_embedding_input_h1 = dense_layer(256, feed_embedding_input, name=\"feed_embedding_input_h1\")\n",
        "    feed_embedding_input_h2 = dense_layer(128, feed_embedding_input_h1, name=\"feed_embedding_input_h2\")\n",
        "    feed_embedding_input_h3 = dense_layer(64, feed_embedding_input_h2, name=\"feed_embedding_input_h3\")\n",
        "    feedid_input, feedid_embedding = embedding_layer(feedid_vocab, num_oov_buckets=240000)\n",
        "    authorid_input, authorid_embedding = embedding_layer(authorid_vocab, num_oov_buckets=40000)\n",
        "    bgm_song_id_input, bgm_song_id_embedding = embedding_layer(bgm_song_id_vocab, num_oov_buckets=60000)\n",
        "    bgm_singer_id_input, bgm_singer_id_embedding = embedding_layer(bgm_singer_id_vocab, num_oov_buckets=40000)\n",
        "    userid_input, userid_embedding = embedding_layer(userid_vocab, num_oov_buckets=40000, output_dim=64)\n",
        "    feed_continuous_features_input = keras.layers.Input(\n",
        "        shape=(len(feed_continuous_features)), \n",
        "        dtype=tf.float32, \n",
        "        name=\"feed_continuous_features_input\"\n",
        "    )\n",
        "    user_continuous_features_input = keras.layers.Input(\n",
        "        shape=(len(user_continuous_features)), \n",
        "        dtype=tf.float32, \n",
        "        name=\"user_continuous_features_input\"\n",
        "    )\n",
        "    device_input = keras.layers.Input(\n",
        "        shape=(2), \n",
        "        name=\"device_input\"\n",
        "    )\n",
        "    feed = keras.layers.concatenate(\n",
        "        [\n",
        "            feed_embedding_input_h3,\n",
        "            feedid_embedding, \n",
        "            authorid_embedding, \n",
        "            bgm_song_id_embedding, \n",
        "            bgm_singer_id_embedding, \n",
        "            feed_continuous_features_input,\n",
        "        ]\n",
        "    )\n",
        "    user = keras.layers.concatenate(\n",
        "        [\n",
        "            device_input,\n",
        "            userid_embedding, \n",
        "            user_continuous_features_input\n",
        "        ]\n",
        "    )\n",
        "    feed_output = dense_layer(256, feed, name=\"feed_h1\", KR=\"l2\")\n",
        "    user_output = dense_layer(256, user, name=\"user_h1\", KR=\"l2\")\n",
        "    output = tf.keras.layers.Dot(axes=1, name=\"dot\")([feed_output, user_output])\n",
        "    inputs=[\n",
        "          feed_continuous_features_input,\n",
        "          user_continuous_features_input,\n",
        "          feedid_input,\n",
        "          authorid_input,\n",
        "          bgm_song_id_input,\n",
        "          bgm_singer_id_input,\n",
        "          userid_input,\n",
        "          device_input,\n",
        "          feed_embedding_input\n",
        "    ]\n",
        "    model = keras.models.Model(\n",
        "      inputs=inputs,\n",
        "      outputs=output\n",
        "    )\n",
        "    return model"
      ],
      "id": "noOiojho_Lio",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4hE-K8U_xHi"
      },
      "source": [
        "##### Deep Cross Network Model"
      ],
      "id": "X4hE-K8U_xHi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDN9ib2g_LqQ",
        "outputId": "c512e18a-a097-4ef6-b3eb-04503c653683"
      },
      "source": [
        "%pip install -q tensorflow-recommenders\n",
        "import tensorflow_recommenders as tfrs"
      ],
      "id": "kDN9ib2g_LqQ",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▉                            | 10 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 20 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 30 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 61 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 81 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 85 kB 2.2 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HLInY95Rg7L"
      },
      "source": [
        "def dcn_model():\n",
        "    keras.backend.clear_session()\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "    feed_embedding_input = keras.layers.Input(shape=[512])\n",
        "    feedid_input, feedid_embedding = embedding_layer(feedid_vocab, num_oov_buckets=240000)\n",
        "    authorid_input, authorid_embedding = embedding_layer(authorid_vocab, num_oov_buckets=40000)\n",
        "    bgm_song_id_input, bgm_song_id_embedding = embedding_layer(bgm_song_id_vocab, num_oov_buckets=60000)\n",
        "    bgm_singer_id_input, bgm_singer_id_embedding = embedding_layer(bgm_singer_id_vocab, num_oov_buckets=40000)\n",
        "    userid_input, userid_embedding = embedding_layer(userid_vocab, num_oov_buckets=40000, output_dim=64)\n",
        "    continuous_features_input = keras.layers.Input(\n",
        "        shape=(len(continuous_features))\n",
        "    )\n",
        "    device_input = keras.layers.Input(\n",
        "        shape=(2)\n",
        "    )\n",
        "    c1 = keras.layers.concatenate(\n",
        "        [\n",
        "            continuous_features_input,\n",
        "            feedid_embedding, \n",
        "            authorid_embedding, \n",
        "            bgm_song_id_embedding, \n",
        "            bgm_singer_id_embedding, \n",
        "            userid_embedding, \n",
        "            device_input,\n",
        "            feed_embedding_input\n",
        "        ]\n",
        "    )\n",
        "    x1 = tfrs.layers.dcn.Cross()(c1, c1)\n",
        "    x2 = tfrs.layers.dcn.Cross()(c1, x1)\n",
        "    x3 = tfrs.layers.dcn.Cross()(c1, x2)\n",
        "    x4 = tfrs.layers.dcn.Cross()(c1, x3)\n",
        "    x5 = tfrs.layers.dcn.Cross()(c1, x4)\n",
        "    x6 = tfrs.layers.dcn.Cross()(c1, x5)\n",
        "    h1 = dense_layer(1024, c1)\n",
        "    h2 = dense_layer(1024, h1, DR=0.50)\n",
        "    c2 = keras.layers.concatenate(\n",
        "        [\n",
        "            x6,\n",
        "            h2\n",
        "        ]\n",
        "    )\n",
        "    output = dense_layer(1, c2, activation=\"sigmoid\")\n",
        "    inputs=[\n",
        "          continuous_features_input,\n",
        "          feedid_input,\n",
        "          authorid_input,\n",
        "          bgm_song_id_input,\n",
        "          bgm_singer_id_input,\n",
        "          userid_input,\n",
        "          device_input,\n",
        "          feed_embedding_input\n",
        "    ]\n",
        "    model = keras.models.Model(\n",
        "      inputs=inputs,\n",
        "      outputs=output\n",
        "    )\n",
        "    return model"
      ],
      "id": "8HLInY95Rg7L",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2uv1lht_1-_"
      },
      "source": [
        "##### Deep Cross Network Version 2 Model"
      ],
      "id": "T2uv1lht_1-_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufVbrQHn_OLB"
      },
      "source": [
        "class crossV2(keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "    def build(self, batch_input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            name=\"kernel\", shape=[int(batch_input_shape[-1]), int(batch_input_shape[-1])],\n",
        "            initializer=\"he_normal\")\n",
        "        self.bias = self.add_weight(\n",
        "            name=\"bias\", shape=[int(batch_input_shape[-1])], initializer=\"zeros\")\n",
        "        super().build(batch_input_shape) \n",
        "    def call(self, X0, X):\n",
        "        return X0*(tf.transpose(self.kernel @ tf.transpose(X)) + self.bias) + X\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [batch_input_shape[-1]])"
      ],
      "id": "ufVbrQHn_OLB",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JmT0N7SAesk"
      },
      "source": [
        "def dcn_v2_model():\n",
        "    keras.backend.clear_session()\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "    feed_embedding_input = keras.layers.Input(shape=[512])\n",
        "    feedid_input, feedid_embedding = embedding_layer(feedid_vocab, num_oov_buckets=240000)\n",
        "    authorid_input, authorid_embedding = embedding_layer(authorid_vocab, num_oov_buckets=40000)\n",
        "    bgm_song_id_input, bgm_song_id_embedding = embedding_layer(bgm_song_id_vocab, num_oov_buckets=60000)\n",
        "    bgm_singer_id_input, bgm_singer_id_embedding = embedding_layer(bgm_singer_id_vocab, num_oov_buckets=40000)\n",
        "    userid_input, userid_embedding = embedding_layer(userid_vocab, num_oov_buckets=40000, output_dim=64)\n",
        "    continuous_features_input = keras.layers.Input(\n",
        "        shape=(len(continuous_features))\n",
        "    )\n",
        "    device_input = keras.layers.Input(\n",
        "        shape=(2)\n",
        "    )\n",
        "    c1 = keras.layers.concatenate(\n",
        "        [\n",
        "            continuous_features_input,\n",
        "            feedid_embedding, \n",
        "            authorid_embedding, \n",
        "            bgm_song_id_embedding, \n",
        "            bgm_singer_id_embedding, \n",
        "            userid_embedding, \n",
        "            device_input,\n",
        "            feed_embedding_input\n",
        "        ]\n",
        "    )\n",
        "    x1 = crossV2()(c1, c1)\n",
        "    x2 = crossV2()(c1, x1)\n",
        "    x3 = crossV2()(c1, x2)\n",
        "    x4 = crossV2()(c1, x3)\n",
        "    x5 = crossV2()(c1, x4)\n",
        "    x6 = crossV2()(c1, x5)\n",
        "    h1 = dense_layer(1024, c1)\n",
        "    h2 = dense_layer(1024, h1, DR=0.50)\n",
        "    c2 = keras.layers.concatenate(\n",
        "        [\n",
        "            x6,\n",
        "            h2\n",
        "        ]\n",
        "    )\n",
        "    output = dense_layer(1, c2, activation=\"sigmoid\", KR=\"l2\")\n",
        "    inputs=[\n",
        "          continuous_features_input,\n",
        "          feedid_input,\n",
        "          authorid_input,\n",
        "          bgm_song_id_input,\n",
        "          bgm_singer_id_input,\n",
        "          userid_input,\n",
        "          device_input,\n",
        "          feed_embedding_input\n",
        "    ]\n",
        "    model = keras.models.Model(\n",
        "      inputs=inputs,\n",
        "      outputs=output\n",
        "    )\n",
        "    return model"
      ],
      "id": "8JmT0N7SAesk",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z595XLq4M3Mp"
      },
      "source": [
        "##### Light Gradient Boosting Tree From Others (When ensembled with DNN, the ensembled model shows a significant improvement )"
      ],
      "id": "z595XLq4M3Mp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1S-_y6RM2p2"
      },
      "source": [
        "# https://developers.weixin.qq.com/community/minihome/article/doc/0006467d05427892b94c341aa56813\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from lightgbm.sklearn import LGBMClassifier\n",
        "from collections import defaultdict\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "pd.set_option('display.max_columns', None)\n",
        "def reduce_mem(df, cols):\n",
        "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    for col in tqdm(cols):\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    gc.collect()\n",
        "    return df\n",
        "## 从官方baseline里面抽出来的评测函数\n",
        "def uAUC(labels, preds, user_id_list):\n",
        "    \"\"\"Calculate user AUC\"\"\"\n",
        "    user_pred = defaultdict(lambda: [])\n",
        "    user_truth = defaultdict(lambda: [])\n",
        "    for idx, truth in enumerate(labels):\n",
        "        user_id = user_id_list[idx]\n",
        "        pred = preds[idx]\n",
        "        truth = labels[idx]\n",
        "        user_pred[user_id].append(pred)\n",
        "        user_truth[user_id].append(truth)\n",
        "    user_flag = defaultdict(lambda: False)\n",
        "    for user_id in set(user_id_list):\n",
        "        truths = user_truth[user_id]\n",
        "        flag = False\n",
        "        # 若全是正样本或全是负样本，则flag为False\n",
        "        for i in range(len(truths) - 1):\n",
        "            if truths[i] != truths[i + 1]:\n",
        "                flag = True\n",
        "                break\n",
        "        user_flag[user_id] = flag\n",
        "    total_auc = 0.0\n",
        "    size = 0.0\n",
        "    for user_id in user_flag:\n",
        "        if user_flag[user_id]:\n",
        "            auc = roc_auc_score(np.asarray(user_truth[user_id]), np.asarray(user_pred[user_id]))\n",
        "            total_auc += auc \n",
        "            size += 1.0\n",
        "    user_auc = float(total_auc)/size\n",
        "    return user_auc\n",
        "y_list = ['read_comment', 'like', 'click_avatar', 'forward', 'favorite', 'comment', 'follow']\n",
        "max_day = 15\n",
        "## 读取训练集\n",
        "train = pd.read_csv('drive/MyDrive/jupyter_notebook/data/wechat_algo_data1/user_action.csv')\n",
        "print(train.shape)\n",
        "for y in y_list:\n",
        "    print(y, train[y].mean())\n",
        "## 读取测试集\n",
        "test = pd.read_csv('drive/MyDrive/jupyter_notebook/data/wechat_algo_data1/test_a.csv')\n",
        "test['date_'] = max_day\n",
        "print(test.shape)\n",
        "## 合并处理\n",
        "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
        "print(df.head(3))\n",
        "## 读取视频信息表\n",
        "feed_info = pd.read_csv(\"drive/MyDrive/jupyter_notebook/data/wechat_algo_data1/feed_info.csv\")\n",
        "## 此份baseline只保留这三列\n",
        "feed_info = feed_info[[\n",
        "    'feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id',\n",
        "]]\n",
        "df = df.merge(feed_info, on='feedid', how='left')\n",
        "## 视频时长是秒，转换成毫秒，才能与play、stay做运算\n",
        "df['videoplayseconds'] *= 1000\n",
        "## 是否观看完视频（其实不用严格按大于关系，也可以按比例，比如观看比例超过0.9就算看完）\n",
        "df['is_finish'] = (df['play'] >= df['videoplayseconds']).astype('int8')\n",
        "df['play_times'] = df['play'] / df['videoplayseconds']\n",
        "play_cols = [\n",
        "    'is_finish', 'play_times', 'play', 'stay'\n",
        "]\n",
        "## 统计历史5天的曝光、转化、视频观看等情况（此处的转化率统计其实就是target encoding）\n",
        "n_day = 5\n",
        "for stat_cols in tqdm([\n",
        "    ['userid'],\n",
        "    ['feedid'],\n",
        "    ['authorid'],\n",
        "    ['userid', 'authorid']\n",
        "]):\n",
        "    f = '_'.join(stat_cols)\n",
        "    stat_df = pd.DataFrame()\n",
        "    for target_day in range(2, max_day + 1):\n",
        "        left, right = max(target_day - n_day, 1), target_day - 1\n",
        "        tmp = df[((df['date_'] >= left) & (df['date_'] <= right))].reset_index(drop=True)\n",
        "        tmp['date_'] = target_day\n",
        "        tmp['{}_{}day_count'.format(f, n_day)] = tmp.groupby(stat_cols)['date_'].transform('count')\n",
        "        g = tmp.groupby(stat_cols)\n",
        "        tmp['{}_{}day_finish_rate'.format(f, n_day)] = g[play_cols[0]].transform('mean')\n",
        "        feats = ['{}_{}day_count'.format(f, n_day), '{}_{}day_finish_rate'.format(f, n_day)]\n",
        "        for x in play_cols[1:]:\n",
        "            for stat in ['max', 'mean']:\n",
        "                tmp['{}_{}day_{}_{}'.format(f, n_day, x, stat)] = g[x].transform(stat)\n",
        "                feats.append('{}_{}day_{}_{}'.format(f, n_day, x, stat))\n",
        "        for y in y_list[:4]:\n",
        "            tmp['{}_{}day_{}_sum'.format(f, n_day, y)] = g[y].transform('sum')\n",
        "            tmp['{}_{}day_{}_mean'.format(f, n_day, y)] = g[y].transform('mean')\n",
        "            feats.extend(['{}_{}day_{}_sum'.format(f, n_day, y), '{}_{}day_{}_mean'.format(f, n_day, y)])\n",
        "        tmp = tmp[stat_cols + feats + ['date_']].drop_duplicates(stat_cols + ['date_']).reset_index(drop=True)\n",
        "        stat_df = pd.concat([stat_df, tmp], axis=0, ignore_index=True)\n",
        "        del g, tmp\n",
        "    df = df.merge(stat_df, on=stat_cols + ['date_'], how='left')\n",
        "    del stat_df\n",
        "    gc.collect()\n",
        "## 全局信息统计，包括曝光、偏好等，略有穿越，但问题不大，可以上分，只要注意不要对userid-feedid做组合统计就行\n",
        "for f in tqdm(['userid', 'feedid', 'authorid']):\n",
        "    df[f + '_count'] = df[f].map(df[f].value_counts())\n",
        "for f1, f2 in tqdm([\n",
        "    ['userid', 'feedid'],\n",
        "    ['userid', 'authorid']\n",
        "]):\n",
        "    df['{}_in_{}_nunique'.format(f1, f2)] = df.groupby(f2)[f1].transform('nunique')\n",
        "    df['{}_in_{}_nunique'.format(f2, f1)] = df.groupby(f1)[f2].transform('nunique')\n",
        "for f1, f2 in tqdm([\n",
        "    ['userid', 'authorid']\n",
        "]):\n",
        "    df['{}_{}_count'.format(f1, f2)] = df.groupby([f1, f2])['date_'].transform('count')\n",
        "    df['{}_in_{}_count_prop'.format(f1, f2)] = df['{}_{}_count'.format(f1, f2)] / (df[f2 + '_count'] + 1)\n",
        "    df['{}_in_{}_count_prop'.format(f2, f1)] = df['{}_{}_count'.format(f1, f2)] / (df[f1 + '_count'] + 1)\n",
        "df['videoplayseconds_in_userid_mean'] = df.groupby('userid')['videoplayseconds'].transform('mean')\n",
        "df['videoplayseconds_in_authorid_mean'] = df.groupby('authorid')['videoplayseconds'].transform('mean')\n",
        "df['feedid_in_authorid_nunique'] = df.groupby('authorid')['feedid'].transform('nunique')\n",
        "## 内存够用的不需要做这一步\n",
        "df = reduce_mem(df, [f for f in df.columns if f not in ['date_'] + play_cols + y_list])\n",
        "train = df[~df['read_comment'].isna()].reset_index(drop=True)\n",
        "test = df[df['read_comment'].isna()].reset_index(drop=True)\n",
        "cols = [f for f in df.columns if f not in ['date_'] + play_cols + y_list]\n",
        "print(train[cols].shape)\n",
        "trn_x = train[train['date_'] < 14].reset_index(drop=True)\n",
        "val_x = train[train['date_'] == 14].reset_index(drop=True)\n",
        "##################### 线下验证 #####################\n",
        "uauc_list = []\n",
        "r_list = []\n",
        "for y in y_list[:4]:\n",
        "    print('=========', y, '=========')\n",
        "    t = time.time()\n",
        "    clf = LGBMClassifier(\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=5000,\n",
        "        num_leaves=63,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=2021,\n",
        "        metric='None'\n",
        "    )\n",
        "    clf.fit(\n",
        "        trn_x[cols], trn_x[y],\n",
        "        eval_set=[(val_x[cols], val_x[y])],\n",
        "        eval_metric='auc',\n",
        "        early_stopping_rounds=100,\n",
        "        verbose=50\n",
        "    )\n",
        "    val_x[y + '_score'] = clf.predict_proba(val_x[cols])[:, 1]\n",
        "    val_uauc = uAUC(val_x[y], val_x[y + '_score'], val_x['userid'])\n",
        "    uauc_list.append(val_uauc)\n",
        "    print(val_uauc)\n",
        "    r_list.append(clf.best_iteration_)\n",
        "    print('runtime: {}\\n'.format(time.time() - t))\n",
        "weighted_uauc = 0.4 * uauc_list[0] + 0.3 * uauc_list[1] + 0.2 * uauc_list[2] + 0.1 * uauc_list[3]\n",
        "print(uauc_list)\n",
        "print(weighted_uauc)\n",
        "##################### 全量训练 #####################\n",
        "r_dict = dict(zip(y_list[:4], r_list))\n",
        "for y in y_list[:4]:\n",
        "    print('=========', y, '=========')\n",
        "    t = time.time()\n",
        "    clf = LGBMClassifier(\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=r_dict[y],\n",
        "        num_leaves=63,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=2021\n",
        "    )\n",
        "    clf.fit(\n",
        "        train[cols], train[y],\n",
        "        eval_set=[(train[cols], train[y])],\n",
        "        early_stopping_rounds=r_dict[y],\n",
        "        verbose=100\n",
        "\n",
        "    )\n",
        "    test[y] = clf.predict_proba(test[cols])[:, 1]\n",
        "    print('runtime: {}\\n'.format(time.time() - t))\n",
        "test[['userid', 'feedid'] + y_list[:4]].to_csv(\n",
        "    'sub_%.6f_%.6f_%.6f_%.6f_%.6f.csv' % (weighted_uauc, uauc_list[0], uauc_list[1], uauc_list[2], uauc_list[3]),\n",
        "    index=False\n",
        ")"
      ],
      "id": "x1S-_y6RM2p2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "invisible-thumbnail"
      },
      "source": [
        "#### Train Model"
      ],
      "id": "invisible-thumbnail"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWuK-EeZ6gud"
      },
      "source": [
        "negative = sum(read_comment_offline_df[\"read_comment\"]==0)\n",
        "positive = sum(read_comment_offline_df[\"read_comment\"]==1)\n",
        "total = len(read_comment_offline_df)\n",
        "negative_weight = (1 / negative) * (total / 2.0)\n",
        "positive_weight = (1 / positive) * (total / 2.0)\n",
        "weight = {0: negative_weight, 1: positive_weight}"
      ],
      "id": "yWuK-EeZ6gud",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "civic-delight"
      },
      "source": [
        "def train_model(n_epochs=4, dataset=None, valid_dataset=None, batch_size=128, model=None, model_name=None, \n",
        "                deep_optimizer=keras.optimizers.Adam(learning_rate=0.01), loss_fn = keras.losses.BinaryCrossentropy(),\n",
        "                metrics = get_metrics([\"precision\", \"recall\"])):\n",
        "    deep_optimizer = deep_optimizer\n",
        "    wide_optimizer = keras.optimizers.Ftrl(learning_rate=0.01)\n",
        "    loss_fn = loss_fn\n",
        "    metrics = metrics\n",
        "    callbacks = get_callbacks([\"earlystopping\"])\n",
        "    if model_name == \"wide_and_deep\":\n",
        "        deep_layers = model.get_layer(\"deep_model\")\n",
        "        wide_layers = model.get_layer(\"wide_model\")\n",
        "        mean_loss = keras.metrics.Mean()\n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "            print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
        "            step = 1\n",
        "            for step_dataset in dataset:\n",
        "                X, y = step_dataset[0], step_dataset[1]\n",
        "                with tf.GradientTape(persistent=True) as tape:\n",
        "                    y_pred = model(X, training=True)\n",
        "                    positive_instances_loss = loss_fn(y, y_pred) * positive_weight * tf.cast(y == 1, dtype=tf.float32)\n",
        "                    negative_instances_loss = loss_fn(y, y_pred) * negative_weight * tf.cast(y == 0, dtype=tf.float32)\n",
        "                    instances_loss = positive_instances_loss + negative_instances_loss\n",
        "                    main_loss = tf.reduce_mean(instances_loss)\n",
        "                    loss = tf.add_n([main_loss] + model.losses)\n",
        "                for layers, optimizer in ((deep_layers, deep_optimizer),\n",
        "                                              (wide_layers, wide_optimizer)):\n",
        "                    gradients = tape.gradient(loss, layers.trainable_variables)\n",
        "                    optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n",
        "                del tape\n",
        "                mean_loss(loss)\n",
        "                for metric in metrics:\n",
        "                    metric(y, y_pred)\n",
        "                print_status_bar(step * batch_size, len(dataset)*batch_size, mean_loss, metrics)\n",
        "                step += 1\n",
        "            print_status_bar(len(dataset)*batch_size, len(dataset)*batch_size, mean_loss, metrics)\n",
        "            for metric in [mean_loss] + metrics:\n",
        "                metric.reset_states()\n",
        "            for step_dataset in valid_dataset:\n",
        "                X, y = step_dataset[0], step_dataset[1]\n",
        "                y_pred = model(X, training=True)\n",
        "                instances_loss = loss_fn(y, y_pred)\n",
        "                loss = tf.reduce_mean(instances_loss)               \n",
        "                mean_loss(loss)\n",
        "                for metric in metrics:\n",
        "                    metric(y, y_pred)\n",
        "            print_status_bar(\"validation\", \"validation\", mean_loss, metrics)\n",
        "            for metric in [mean_loss] + metrics:\n",
        "                metric.reset_states()\n",
        "    else:\n",
        "        model.compile(optimizer=deep_optimizer, loss=loss_fn, metrics=metrics)\n",
        "        model.fit(dataset, validation_data=valid_dataset, callbacks=callbacks, epochs=n_epochs, class_weight=weight) "
      ],
      "id": "civic-delight",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyQ9DvOI0giY"
      },
      "source": [
        "my_wide_and_deep_model = wide_and_deep_model()\n",
        "my_fm_model = fm_model()\n",
        "my_dcn_model = dcn_model()\n",
        "my_dcn_v2_model = dcn_v2_model()"
      ],
      "id": "hyQ9DvOI0giY",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KxhTd7C_Z-F"
      },
      "source": [
        "dataset = get_dataset(\"train\", read_comment_offline_df)\n",
        "valid_dataset = get_dataset(\"valid\", evaluate_df)\n",
        "fm_dataset = get_dataset(\"train\", read_comment_offline_df, model=\"fm\")\n",
        "fm_valid_dataset = get_dataset(\"valid\", read_comment_offline_df, model=\"fm\")\n",
        "test_dataset = get_dataset(\"test\", evaluate_df, test=True)\n",
        "fm_test_dataset = get_dataset(\"test\", evaluate_df, test=True, model=\"fm\")"
      ],
      "id": "2KxhTd7C_Z-F",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhhAkn7q_Tqh",
        "outputId": "06bd399f-082e-40ae-ba34-16e342eb4a96"
      },
      "source": [
        "train_model(n_epochs=4, dataset=dataset, valid_dataset=valid_dataset, batch_size=128, model=my_wide_and_deep_model, model_name=\"wide_and_deep\")"
      ],
      "id": "RhhAkn7q_Tqh",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "619776/619776 - mean: 0.2354 - precision: 0.6979 - recall: 0.6084\n",
            "619776/619776 - mean: 0.2354 - precision: 0.6979 - recall: 0.6084\n",
            "validation/validation - mean: 0.1446 - precision: 0.2881 - recall: 0.6076\n",
            "Epoch 2/4\n",
            "619776/619776 - mean: 0.1995 - precision: 0.7194 - recall: 0.6892\n",
            "619776/619776 - mean: 0.1995 - precision: 0.7194 - recall: 0.6892\n",
            "validation/validation - mean: 0.1540 - precision: 0.2781 - recall: 0.6250\n",
            "Epoch 3/4\n",
            "619776/619776 - mean: 0.1724 - precision: 0.7321 - recall: 0.7484\n",
            "619776/619776 - mean: 0.1724 - precision: 0.7321 - recall: 0.7484\n",
            "validation/validation - mean: 0.1744 - precision: 0.2451 - recall: 0.6806\n",
            "Epoch 4/4\n",
            "619776/619776 - mean: 0.1543 - precision: 0.7414 - recall: 0.7893\n",
            "619776/619776 - mean: 0.1543 - precision: 0.7414 - recall: 0.7893\n",
            "validation/validation - mean: 0.1898 - precision: 0.2274 - recall: 0.7017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPWQlxpx8Y8e",
        "outputId": "c509ea82-79ec-476e-b3cd-4f9468c38182"
      },
      "source": [
        "train_model(n_epochs=4, dataset=fm_dataset, valid_dataset=fm_valid_dataset, model=my_fm_model, deep_optimizer=keras.optimizers.Adam(learning_rate=0.001), loss_fn=keras.losses.mse, metrics=[]) "
      ],
      "id": "fPWQlxpx8Y8e",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "4842/4842 [==============================] - 379s 78ms/step - loss: 1.2791 - val_loss: 0.1642\n",
            "Epoch 2/4\n",
            "4842/4842 [==============================] - 378s 78ms/step - loss: 0.1173 - val_loss: 0.1041\n",
            "Epoch 3/4\n",
            "4842/4842 [==============================] - 381s 79ms/step - loss: 0.0795 - val_loss: 0.0967\n",
            "Epoch 4/4\n",
            "4842/4842 [==============================] - 379s 78ms/step - loss: 0.0690 - val_loss: 0.0983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s4yVvdZ8gpo",
        "outputId": "fff2a72c-31e0-4933-e3ae-35c6c2b00945"
      },
      "source": [
        "train_model(n_epochs=8, dataset=dataset, valid_dataset=valid_dataset, model=my_dcn_model, deep_optimizer=keras.optimizers.Adam(learning_rate=0.001))"
      ],
      "id": "2s4yVvdZ8gpo",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "4842/4842 [==============================] - 628s 129ms/step - loss: 0.3178 - precision: 0.5174 - recall: 0.8954 - val_loss: 0.3474 - val_precision: 0.1649 - val_recall: 0.8677\n",
            "Epoch 2/8\n",
            "4842/4842 [==============================] - 629s 130ms/step - loss: 0.2438 - precision: 0.5642 - recall: 0.9338 - val_loss: 0.3583 - val_precision: 0.1512 - val_recall: 0.8621\n",
            "Epoch 3/8\n",
            "4842/4842 [==============================] - 627s 129ms/step - loss: 0.1657 - precision: 0.6636 - recall: 0.9651 - val_loss: 0.3995 - val_precision: 0.1413 - val_recall: 0.8433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQFY4TpN8epk",
        "outputId": "7d4d6a3c-e241-4871-d304-6a1fbb13a623"
      },
      "source": [
        "train_model(n_epochs=8, dataset=dataset, valid_dataset=valid_dataset, model=my_dcn_v2_model, deep_optimizer=keras.optimizers.Adam(learning_rate=0.001))"
      ],
      "id": "zQFY4TpN8epk",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "4842/4842 [==============================] - 638s 131ms/step - loss: 0.3193 - precision: 0.3679 - recall: 0.8896 - val_loss: 0.3611 - val_precision: 0.1614 - val_recall: 0.8733\n",
            "Epoch 2/8\n",
            "4842/4842 [==============================] - 633s 131ms/step - loss: 0.2482 - precision: 0.5603 - recall: 0.9333 - val_loss: 0.3924 - val_precision: 0.1453 - val_recall: 0.8696\n",
            "Epoch 3/8\n",
            "4842/4842 [==============================] - 633s 131ms/step - loss: 0.1742 - precision: 0.6523 - recall: 0.9651 - val_loss: 0.3934 - val_precision: 0.1403 - val_recall: 0.8282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cooperative-insured"
      },
      "source": [
        "#### Evaluate Model"
      ],
      "id": "cooperative-insured"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dutch-zoning"
      },
      "source": [
        "userid_list = evaluate_df[\"userid\"].astype(str).tolist()\n",
        "labels = evaluate_df[\"read_comment\"].tolist()\n",
        "def uauc_evaluate(model, model_name, test_dataset):\n",
        "    def predict(model, test_dataset):\n",
        "        all_predictions = None\n",
        "        all_predictions_flag = False\n",
        "        for x in test_dataset:\n",
        "            batch_predictions = model(x)\n",
        "            if not all_predictions_flag:\n",
        "                all_predictions = batch_predictions\n",
        "                all_predictions_flag = True\n",
        "            else:\n",
        "                all_predictions = np.row_stack([all_predictions, batch_predictions])\n",
        "        return all_predictions\n",
        "    predictions = predict(model, test_dataset)\n",
        "    print(f\"{model_name} uAUC:{uAUC(labels, predictions, userid_list)}\")"
      ],
      "id": "dutch-zoning",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TixG7uYY1f7",
        "outputId": "9820d2d7-de02-4a12-ae91-3a1ce39308aa"
      },
      "source": [
        "print(uauc_evaluate(my_wide_and_deep_model, \"my_wide_and_deep_model\", test_dataset))"
      ],
      "id": "_TixG7uYY1f7",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_wide_and_deep_model uAUC:0.5802043553435963\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM5lUEntsD5H",
        "outputId": "12d049a9-0e3c-46d5-d298-6654a6a52bdf"
      },
      "source": [
        "print(uauc_evaluate(my_fm_model, \"my_fm_model\", fm_test_dataset))"
      ],
      "id": "dM5lUEntsD5H",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_fm_model uAUC:0.5986379389857975\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc7BK2Uk3EsE",
        "outputId": "7c82aa5f-8bde-44f0-9c16-a6ece36a355a"
      },
      "source": [
        "print(uauc_evaluate(my_dcn_model, \"my_dcn_model\", test_dataset))"
      ],
      "id": "Tc7BK2Uk3EsE",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_dcn_model uAUC:0.6144456315323304\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqmC8_uITHgb",
        "outputId": "b335234c-e87a-426e-c522-af5cf5c344a0"
      },
      "source": [
        "print(uauc_evaluate(my_dcn_v2_model, \"my_dcn_v2_model\", test_dataset))"
      ],
      "id": "vqmC8_uITHgb",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_dcn_v2_model uAUC:0.616542769765119\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}